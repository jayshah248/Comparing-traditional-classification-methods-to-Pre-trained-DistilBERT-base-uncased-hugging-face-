# -*- coding: utf-8 -*-
"""Final_data_pipe_line_with_data_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13KQpI8WZLTolK_wJ6_r9jivZcag27GhR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
sns.set_style('darkgrid')

from wordcloud import WordCloud, STOPWORDS
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn import feature_extraction
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')
from bs4 import BeautifulSoup

import os
import warnings
import torch
import numpy as np 
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
seed = 42
sns.set_theme()
pd.set_option('display.max_colwidth', None)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
model_name = "distilbert-base-uncased"
TRAIN_FILE = "/content/drive/MyDrive/Machine learning ML_robinson final project/nlp_disaster_trian_data.csv"

def clean_text(text):
    """Pre-process text and generate tokens

    Args:
        text: Text to tokenize.

    Returns:
        Tokenized text.
    """
    text = str(text).lower()  # Lowercase words
    text = BeautifulSoup(text, 'html.parser').get_text() # Remove html tags
    text = re.sub("http[s]?\:\/\/\S+", " ", text)
    text = re.sub(r"\[(.*?)\]", "", text)  # Remove [+XYZ chars] in content
    text = re.sub(r"\s+", " ", text)  # Remove multiple spaces in content
    text = re.sub(r"\w+…|…", "", text)  # Remove ellipsis (and last word)
    text = re.sub(r"(?<=\w)-(?=\w)", " ", text)  # Replace dash between words
    return text

def f1(pred, y):
    score = f1_score(y,pred)
    return 'f1', score

train = pd.read_csv(TRAIN_FILE)
train['cleaned_text'] = train['text'].apply(lambda x: clean_text(x))

len(train)

fig, ax = plt.subplots(1, 2, figsize=(12, 4))

train["target"].value_counts().plot(
    kind="pie", 
    explode=[0.05 for x in train["target"].unique()], 
    autopct='%.2f%%',
    ax=ax[0], 
    shadow=True 
)
ax[0].set_title(f"Target Distribution Pie Chart")
ax[0].set_ylabel('')

count = sns.countplot(x="target", data=train, ax=ax[1],palette="Blues_d")
for bar in count.patches:
    count.annotate(format(bar.get_height()),
        (bar.get_x() + bar.get_width() / 2,
        bar.get_height()), ha='center', va='center',
        size=11, xytext=(0, 8),
        textcoords='offset points')
ax[1].set_title(f"Tweet distribution chart")
plt.show()

train["Tweet_length_2"] = train["text"].apply(lambda x: len(x))
train.sort_values(by="Tweet_length_2", ascending=False)

train["Tweet_length"] = train["cleaned_text"].apply(lambda x: len(x))
train.sort_values(by="Tweet_length", ascending=False)

sns.displot(data=train, x="Tweet_length")
plt.title("Distribution pattern")
plt.show()

print(f"\nLowest tweet length: {train['Tweet_length'].min()}")
print(f"Highest tweet length: {train['Tweet_length'].max()}")

non_disaster_tweet=train[train["target"]==0]
disaster_tweet=train[train["target"]==1]

sns.displot(data=disaster_tweet, x="Tweet_length")
plt.title("Disaster tweet lenght")
plt.show()

print(f"\nLowest tweet length: {disaster_tweet['Tweet_length'].min()}")
print(f"Highest tweet length: {disaster_tweet['Tweet_length'].max()}")

sns.displot(data=non_disaster_tweet, x="Tweet_length")
plt.title("Non-disaster tweet lenght")
plt.show()

print(f"\nLowest tweet length: {non_disaster_tweet['Tweet_length'].min()}")
print(f"Highest tweet length: {non_disaster_tweet['Tweet_length'].max()}")

train.hist(column='Tweet_length', by = 'target',bins =30, figsize= (10,5))

train["Tweet_length"].describe(percentiles=[.25, .5, .75, .90, .95, .99])

X=train["cleaned_text"]
y=train["target"]
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

len(X_train)

len(X_test)

from sklearn.ensemble import AdaBoostClassifier

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os

# NLP libraries
import string # Library for string operations
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import re # Regex library
from wordcloud import WordCloud # Word Cloud library

# ploting libraries
import matplotlib.pyplot as plt

# ML/AI libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm

vectorizer = TfidfVectorizer(min_df = 0.0005, 
                             max_features = 100000, 
                             tokenizer = lambda x: x.split(),
                             ngram_range = (1,4))

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

Model = svm.SVC(kernel='linear')
Model.fit(X_train, y_train)
y_pred = Model.predict(X_test)
pred = y_pred.round().astype('int32')

current_fold_score = f1_score(y_test,pred)
current_fold_score

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test,pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

Accuracy = metrics.accuracy_score(y_test,pred)
print(Accuracy)

clf = AdaBoostClassifier(n_estimators=1000, random_state=2)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
pred = y_pred.round().astype('int32')
current_fold_score = f1_score(y_test,pred)
current_fold_score



class Cross_valid:
    def __init__(self,train_data,n_splits):
        self.best_score = 0
        self.best_model = list()
        self.best_model_std = 0
        self.feature_cols = None
        no_classes = 2
        self.actual_classes = np.empty([0], dtype=int)
        self.predicted_classes = np.empty([0], dtype=int)
        self.predicted_proba = np.empty([0, no_classes])
        
        train_data['kfold'] = -1

        kf = KFold(n_splits=n_splits,shuffle=True)
        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_data)):
            train_data.loc[valid_idx,'kfold'] = fold
        
        self.data = train_data
        self.n_splits = n_splits
        
    
    def run_model(self, pipeline_steps, model, feature_cols, target_col, valid_pipeline = False):
        data = self.data.copy()
        current_model_scores = np.array([])
        print(f' {str(model)} model...\n')
        for fold in range(self.n_splits):
            x_train = data[data['kfold'] != fold][feature_cols].copy()
            x_valid = data[data['kfold'] == fold][feature_cols].copy()
            
            y_train = data[data['kfold'] != fold][target_col].copy()
            y_valid = data[data['kfold'] == fold][target_col].copy()
            
            x_train_pipelined = Pipeline(steps=pipeline_steps).fit_transform(x_train)
            x_valid_pipelined = Pipeline(steps=pipeline_steps).transform(x_valid)
                
            

            
            model.fit(x_train_pipelined,y_train)
            prediction_valid = model.predict(x_valid_pipelined)
            
            current_fold_score = f1_score(y_valid,prediction_valid)
            current_model_scores = np.append(current_model_scores,current_fold_score)
            confusion_matrix = metrics.confusion_matrix(y_valid,prediction_valid)
            cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
            cm_display.plot()
            plt.show()
            print(f'Fold {fold} validation score: {current_fold_score}')
        
        
        avg_score = current_model_scores.mean()
        std = current_model_scores.std()
        print(f' {str(model)} model score is: {round(avg_score,3)} \nStandard deviation: {round(std,3)}')

tfidf_pipeline = [('cv',feature_extraction.text.TfidfVectorizer(stop_words='english',max_features=1000,ngram_range=(1,3)))]
Model = svm.SVC(kernel='linear')
five_fold = Cross_valid(train,3)
five_fold.run_model(tfidf_pipeline,Model,feature_cols = 'cleaned_text',target_col = 'target')

lr = LogisticRegression()
five_fold.run_model(tfidf_pipeline,lr,feature_cols = 'cleaned_text',target_col = 'target')

xgb = XGBClassifier(n_jobs=4,maximize=True, feval=f1)
five_fold.run_model(tfidf_pipeline,xgb,feature_cols = 'cleaned_text',target_col = 'target')

from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
five_fold.run_model(tfidf_pipeline,nb,feature_cols = 'cleaned_text',target_col = 'target')

